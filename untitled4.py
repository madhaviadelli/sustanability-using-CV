# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15WgaEdx0oHKDBJu_msgqiZMIfcHBd25G
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from  scipy import stats

with open('/content/README.md', 'r') as f:
    readme_content = f.read()

print(readme_content)

import re
import pandas as pd

citation_content = sections_content.get('Citing WaRP dataset', [])
citation_text = "\n".join(citation_content)

# Regex to extract BibTeX fields
# This pattern looks for the field name (e.g., title, author) and its value within curly braces
pattern = r"@article\{.*?,\s*(.*?)\s*=\s*\{(.*?)\}"

matches = re.findall(pattern, citation_text, re.DOTALL)

citation_data = []
for match in matches:
    field_name = match[0].strip()
    field_value = match[1].strip()
    citation_data.append({"Field": field_name, "Value": field_value})

# Create a pandas DataFrame
citation_df = pd.DataFrame(citation_data)

print("Citation Information Table:")
display(citation_df)

display(categories_df)

import pandas as pd

# Access the content of the relevant sections
warp_d_content = sections_content.get('Warp-D Detection', [])
warp_c_content = sections_content.get('Warp-C Classification', [])
warp_s_content = sections_content.get('Warp-S Segmentation', [])

# Extract key information from each section's content
warp_d_info = {
    "Part": "WaRP-D",
    "Purpose": "Detection",
    "Training Images": "2452",
    "Validation Images": "522",
    "Image Resolution": "1920 x 1080 pixels",
    "Annotation": ".txt annotation with bboxes"
}

# Extracting details for WaRP-C from its content
warp_c_info = {
    "Part": "WaRP-C",
    "Purpose": "Classification",
    "Training Images": "8823",
    "Testing Images": "1583",
    "Image Size Range": "40 to 703 pixels wide and 35 to 668 pixels high",
    "Characteristics": "Unbalanced dataset, cut-out image areas from WaRP-D"
}

# Extracting details for WaRP-S from its content
warp_s_info = {
    "Part": "WaRP-S",
    "Purpose": "Segmentation",
    "Total Images": "112",
    "Image Size Range": "100 × 96 pixels to 412 × 510 pixels",
    "Characteristics": "Each category has 4 images with significantly deformed objects"
}


# Create a list of dictionaries for the DataFrame
dataset_parts_data = [warp_d_info, warp_c_info, warp_s_info]

# Create a pandas DataFrame
dataset_parts_df = pd.DataFrame(dataset_parts_data)

# Display the DataFrame
print("Dataset Parts Summary:")
display(dataset_parts_df)

import re
import pandas as pd

text = """Examples of instances of each category of the WaRP Dataset are presented in the figure below.

![Dataset classes](/assets/WaRP-Categories.png)
![Dataset parts](/assets/WaRP-Dataset.png)
"""

# Regex to find markdown images: ![alt text](image_path)
# It captures the alt text and the image path
pattern = r"!\[(.*?)\]\((.*?)\)"

matches = re.findall(pattern, text)

image_data = []
for match in matches:
    alt_text = match[0]
    image_path = match[1]
    image_data.append({"Alt Text": alt_text, "Image Path": image_path})

# Create a pandas DataFrame from the extracted image data
images_df = pd.DataFrame(image_data)

print("Image Information Table:")
display(images_df)

print(len(readme_content))

df=readme_content

sections = []
with open('/content/README.md', 'r') as f:
    for line in f:
        line = line.strip()
        if line.startswith('#'):
            # Assuming headings start with '#' and are followed by a space
            # We strip leading '#' and spaces and add to the list
            section_title = line.lstrip('# ').strip()
            if section_title: # Ensure it's not just a line with '#'
                sections.append(section_title)

print("Extracted sections:")
for section in sections:
    print(section)

import pandas as pd

sections_df = pd.DataFrame(sections, columns=['Titles)'])
display(sections_df.head())

# Check for duplicate section titles in sections_df
duplicate_sections = sections_df.duplicated()
print("Rows with duplicate section titles:")
display(sections_df[duplicate_sections])

# To count the number of duplicate section titles:
print(f"\nNumber of duplicate section titles: {duplicate_sections.sum()}")

sections_df['Title Length'] = sections_df['Titles)'].apply(len)
display(sections_df)

df=pd.read_csv("/content/sections.csv")

# Remove the 'Title Length' column
sections_df = sections_df.drop(columns=['Title Length'])

# Display the DataFrame after removing the column
display(sections_df)

# Load data from the CSV file into sections_df
sections_df = pd.read_csv('/content/sections.csv')

# Display the head of the updated sections_df DataFrame
display(sections_df.head())

